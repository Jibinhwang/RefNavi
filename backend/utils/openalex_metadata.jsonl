{"title": "Layer Normalization", "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.", "doi": "https://doi.org/10.48550/arxiv.1607.06450", "year": 2016, "authors": ["Jimmy Ba", "Jamie Kiros", "Geoffrey E. Hinton"], "citation_count": 1320}
{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "abstract": "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "doi": null, "year": 2015, "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "citation_count": 15333}
{"title": "Massive Exploration of Neural Machine Translation Architectures", "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.", "doi": "https://doi.org/10.18653/v1/d17-1151", "year": 2017, "authors": ["Denny Britz", "Anna Goldie", "Minh-Thang Luong", "Quoc V. Le"], "citation_count": 463}
{"title": "Long Short-Term Memory-Networks for Machine Reading", "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input.We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention.The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell.This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens.The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture.Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.", "doi": "https://doi.org/10.18653/v1/d16-1053", "year": 2016, "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "citation_count": 1024}
{"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "doi": "https://doi.org/10.48550/arxiv.1406.1078", "year": 2014, "authors": ["Kyunghyun Cho", "Bart van Merriënboer", "Çağlar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "citation_count": 2978}
