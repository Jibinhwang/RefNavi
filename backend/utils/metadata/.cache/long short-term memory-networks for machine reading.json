{
  "title": "Long Short-Term Memory-Networks for Machine Reading",
  "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input.We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention.The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell.This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens.The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture.Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.",
  "doi": "https://doi.org/10.18653/v1/d16-1053",
  "year": 2016,
  "authors": [
    "Jianpeng Cheng",
    "Li Dong",
    "Mirella Lapata"
  ],
  "citation_count": 1024,
  "source": "openalex"
}