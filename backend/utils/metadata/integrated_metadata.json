{
  "title": "Attention Is All You Need",
  "abstract_original": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
  "abstract_llm": "The paper introduces the Transformer, a model architecture that replaces recurrence with an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. The Transformer outperforms all previously reported models on the WMT 2014 English-to-German and English-to-French translation tasks.",
  "body_fixed": "1 Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38,24,15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h, as a function of the previous hidden state h and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2,19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4,27,28,22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5,2,35]. Here, the encoder maps an input sequence of symbol representations (x,...,x) to a sequence of continuous representations z = (z,...,z). Given z, the decoder then generates an output sequence (y,...,y) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 2 Figure 1: The Transformer-model architecture. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d = 512. model Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n\nWe also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as a mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d, and values of dimension d. We compute the dot products of the query with all keys, divide each by √d, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as: QKT Attention(Q,K,V)=softmax( )V (1) √d k The two most commonly used attention functions are additive attention[2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of d the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d[3]. We suspect that for large values of d, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients4. To counteract this effect, we scale the dot products by 1. √dk 3.2.2 Multi-Head Attention Instead of performing a single attention function with d-dimensional keys, values and queries, model we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d, d and d dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. 4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q·k=P d i= k 1 qiki, has mean 0 and variance dk. 4 Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h where head =Attention(QWQ,KWK,VWV) i i i i W an h d e W re O the p R ro h je d c v t × io d n mo s de a l. re parameter matrices W i Q ∈ Rd model × dk, W i K ∈ Rd model × dk, W i V ∈ Rd model × dv ∈ In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d =d =d /h=64. Due to the reduced dimension of each head, the total computational cost k v model is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to ) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 While the linear transformations are the same across different positions, they used different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d =2048. ff 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √d. model 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r is the size of the neighborhood in restricted self-attention.\n\nLayer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k n · d2) O(1) O(log (n)) k · · Self-Attention(restricted) O(r n d) O(1) O(n/r) · · tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottom so of the encoder and decoder stacks. The positional encoding have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed[9]. In this work, we use sine and cosine functions of different frequencies: PE = sin(pos/100002i/d model) (pos,2i) PE = cos(pos/100002i/d model) (pos,2i+1) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE can be represented as a linear function of pos+k PE . pos We also experimented with using learned positional embeddings[9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z Rd, such as a hidden 1 n 1 n i i ∈ layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backwards signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies[12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k <n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log (n)) in the case of dilated convolutions[18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k n d+n d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding[3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary[38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer[20] with β =0.9, β =0.98 and ǫ=10 9. We varied the learning 1 2 − rate over the course of training, according to the formula: lrate=d m−o 0 d . e 5 l· min(step_num − 0.5, step_num · warmup_steps − 1.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps=4000. 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P =0.1.\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French news test 2014 tests at a fraction of the training cost. BLEU Training Cost (FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[18] 23.75 Deep-Att+PosUnk[39] 39.2 1.0 1020 GNMT+RL[38] 24.6 39.92 2.3 1019 1.4 · 1020 ConvS2S[9] 25.16 40.46 9.6 · 1018 1.5 · 1020 MoE[32] 26.03 40.56 2.0 · 1019 1.2 · 1020 · · Deep-Att+PosUnkEnsemble[39] 40.4 8.0 1020 GNMT+RLEnsemble[38] 26.30 41.16 1.8 1020 1.1 · 1021 ConvS2SEnsemble[9] 26.36 41.29 7.7 · 1019 1.2 · 1021 · · Transformer (base model) 27.3 38.1 3.3·1018 Transformer (big) 28.4 41.8 2.3 1019 · Label Smoothing During training, we employed label smoothing of value ǫ = 0.1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length+50, but terminate early when possible[38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, news test 2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. 5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, news test 2013. Listed perplexities are per-word piece, according to our byte-pair encoding, and should not be compared to per-word perplexities. train PPL BLEU params N d d h d d P ǫ model ff k v drop ls steps (dev) (dev) 106 × base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 4 128 128 5.00 25.5 (A) 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 (B) 32 5.01 25.4 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 (D) 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ23F1 Vinyals & Kaiser el al. (2014)[37] WSJ only, discriminative 88.3 Petrov et al. (2006)[29] WSJ only, discriminative 90.4 Zhu et al. (2013)[40] WSJ only, discriminative 90.4 Dyer et al. (2016)[8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. (2013)[40] semi-supervised 91.3 Huang & Harper (2009)[14] semi-supervised 91.3 McClosky et al. (2006)[26] semi-supervised 92.1 Vinyals & Kaiser el al. (2014)[37] semi-supervised 92.1 Transformer (4 layers) semi-supervised 92.7 Luong et al. (2015)[23] multi-task 93.0 Dyer et al. (2016)[8] generative 93.3 In Table 3 rows (B), we observe that reducing the attention key size hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings[9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes[37]. We trained a 4-layer transformer with d = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank[25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and Berkley Parser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length+300.\n\nWe used a beam size of 21 and α=0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar. In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley Parser even when training only on the WSJ training set of 40K sentences. In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goal of ours. The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor. We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.",
  "references": [
    {
      "ref_number": "[1]",
      "ref_title": "Layer normalization",
      "citation_contexts": [
        "We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]."
      ],
      "title": "Layer Normalization",
      "abstract": "Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
      "doi": null,
      "year": 2016,
      "authors": [
        "Jimmy Ba",
        "J. Kiros",
        "Geoffrey E. Hinton"
      ],
      "citation_count": 10476,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[2]",
      "ref_title": "Neural machine translation by jointly learning to align and translate",
      "citation_contexts": [
        "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5].",
        "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2,19].",
        "Most competitive neural sequence transduction models have an encoder-decoder structure [5,2,35].",
        "The two most commonly used attention functions are additive attention[2], and dot-product (multiplicative) attention.",
        "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]."
      ],
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
      "doi": null,
      "year": 2014,
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "citation_count": 27276,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[3]",
      "ref_title": "Massive exploration of neural machine translation architectures",
      "citation_contexts": [
        "While for small values of d the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d[3].",
        "Sentences were encoded using byte-pair encoding[3], which has a shared source-target vocabulary of about 37000 tokens."
      ],
      "title": "Massive Exploration of Neural Machine Translation Architectures",
      "abstract": "Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.",
      "doi": "10.18653/v1/D17-1151",
      "year": 2017,
      "authors": [
        "D. Britz",
        "Anna Goldie",
        "Minh-Thang Luong",
        "Quoc V. Le"
      ],
      "citation_count": 519,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[4]",
      "ref_title": "Long short-term memory-networks for machine reading",
      "citation_contexts": [
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4,27,28,22]."
      ],
      "title": "Long Short-Term Memory-Networks for Machine Reading",
      "abstract": "In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.",
      "doi": "10.18653/v1/D16-1053",
      "year": 2016,
      "authors": [
        "Jianpeng Cheng",
        "Li Dong",
        "Mirella Lapata"
      ],
      "citation_count": 1118,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[5]",
      "ref_title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "citation_contexts": [
        "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5].",
        "Most competitive neural sequence transduction models have an encoder-decoder structure [5,2,35]."
      ],
      "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
      "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "doi": "10.3115/v1/D14-1179",
      "year": 2014,
      "authors": [
        "Kyunghyun Cho",
        "B. V. Merrienboer",
        "Çaglar Gülçehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "citation_count": 23325,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[6]",
      "ref_title": "Xception: Deep learning with depthwise separable convolutions",
      "citation_contexts": [
        "Separable convolutions [6], however, decrease the complexity considerably, to O(k n d+n d2)."
      ],
      "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
      "doi": "10.1109/CVPR.2017.195",
      "year": 2016,
      "authors": [
        "François Chollet"
      ],
      "citation_count": 14543,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[7]",
      "ref_title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "citation_contexts": [
        "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5]."
      ],
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.",
      "doi": null,
      "year": 2014,
      "authors": [
        "Junyoung Chung",
        "Çaglar Gülçehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "citation_count": 12692,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[8]",
      "ref_title": "Recurrent neural network grammars",
      "citation_contexts": [
        "Dyer et al. (2016)[8] WSJ only, discriminative 91.7",
        "Dyer et al. (2016)[8] generative 93.3",
        "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar."
      ],
      "title": "Recurrent Neural Network Grammars",
      "abstract": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.",
      "doi": "10.18653/v1/N16-1024",
      "year": 2016,
      "authors": [
        "Chris Dyer",
        "A. Kuncoro",
        "Miguel Ballesteros",
        "Noah A. Smith"
      ],
      "citation_count": 526,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[9]",
      "ref_title": "Convolutional sequence to sequence learning",
      "citation_contexts": [
        "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.",
        "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9].",
        "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9].",
        "There are many choices of positional encodings, learned and fixed[9]. In this work, we use sine and cosine functions of different frequencies.",
        "We also experimented with using learned positional embeddings[9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)).",
        "ConvS2S[9] 25.16 40.46 9.6 · 1018 1.5 · 1020",
        "ConvS2SEnsemble[9] 26.36 41.29 7.7 · 1019 1.2 · 1021",
        "In row (E) we replace our sinusoidal positional encoding with learned positional embeddings[9], and observe nearly identical results to the base model."
      ],
      "title": "Convolutional Sequence to Sequence Learning",
      "abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Jonas Gehring",
        "Michael Auli",
        "David Grangier",
        "Denis Yarats",
        "Yann Dauphin"
      ],
      "citation_count": 3284,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[10]",
      "ref_title": "Generating sequences with recurrent neural networks",
      "citation_contexts": [
        "At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next."
      ],
      "title": "Generating Sequences With Recurrent Neural Networks",
      "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
      "doi": null,
      "year": 2013,
      "authors": [
        "Alex Graves"
      ],
      "citation_count": 4032,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[11]",
      "ref_title": "Deep residual learning for image recognition",
      "citation_contexts": [
        "We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]."
      ],
      "title": "Deep Residual Learning for Image Recognition",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "doi": "10.1109/cvpr.2016.90",
      "year": 2015,
      "authors": [
        "Kaiming He",
        "X. Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "citation_count": 193641,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[12]",
      "ref_title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies",
      "citation_contexts": [
        "In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12].",
        "The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies[12]."
      ],
      "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies",
      "abstract": null,
      "doi": null,
      "year": 2001,
      "authors": [
        "Sepp Hochreiter",
        "Yoshua Bengio"
      ],
      "citation_count": 1978,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[13]",
      "ref_title": "Long short-term memory",
      "citation_contexts": [
        "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5]."
      ],
      "title": "Long Short-Term Memory",
      "abstract": null,
      "doi": "10.1162/neco.1997.9.8.1735",
      "year": 1997,
      "authors": [
        "Sepp Hochreiter",
        "J. Schmidhuber"
      ],
      "citation_count": 90540,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[14]",
      "ref_title": "Self-training PCFG grammars with latent annotations across languages",
      "citation_contexts": [
        "Huang & Harper (2009)[14] semi-supervised 91.3"
      ],
      "title": "Self-Training PCFG Grammars with Latent Annotations Across Languages",
      "abstract": "We investigate the effectiveness of self-training PCFG grammars with latent annotations (PCFG-LA) for parsing languages with different amounts of labeled training data. Compared to Charniak's lexicalized parser, the PCFG-LA parser was more effectively adapted to a language for which parsing has been less well developed (i.e., Chinese) and benefited more from self-training. We show for the first time that self-training is able to significantly improve the performance of the PCFG-LA parser, a single generative parser, on both small and large amounts of labeled training data. Our approach achieves state-of-the-art parsing accuracies for a single parser on both English (91.5%) and Chinese (85.2%).",
      "doi": "10.3115/1699571.1699621",
      "year": 2009,
      "authors": [
        "Zhongqiang Huang",
        "M. Harper"
      ],
      "citation_count": 104,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[15]",
      "ref_title": "Exploring the limits of language modeling",
      "citation_contexts": [
        "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38,24,15]."
      ],
      "title": "Exploring the Limits of Language Modeling",
      "abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",
      "doi": null,
      "year": 2016,
      "authors": [
        "R. Józefowicz",
        "O. Vinyals",
        "M. Schuster",
        "Noam M. Shazeer",
        "Yonghui Wu"
      ],
      "citation_count": 1145,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[16]",
      "ref_title": "Can active memory replace attention?",
      "citation_contexts": [
        "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions."
      ],
      "title": "Can Active Memory Replace Attention?",
      "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.",
      "doi": null,
      "year": 2016,
      "authors": [
        "Lukasz Kaiser",
        "Samy Bengio"
      ],
      "citation_count": 59,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[17]",
      "ref_title": "Neural GPUs learn algorithms",
      "citation_contexts": [
        "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9]."
      ],
      "title": "Neural GPUs Learn Algorithms",
      "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. \nWe present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. \nAn essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. \nTo achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.",
      "doi": null,
      "year": 2015,
      "authors": [
        "Lukasz Kaiser",
        "I. Sutskever"
      ],
      "citation_count": 369,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[18]",
      "ref_title": "Neural machine translation in linear time",
      "citation_contexts": [
        "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.",
        "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17,18] and [9].",
        "Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log (n)) in the case of dilated convolutions[18], increasing the length of the longest paths between any two positions in the network.",
        "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French news test 2014 tests at a fraction of the training cost. BLEU Training Cost (FLOPs) Model EN-DE EN-FR EN-DE EN-FR ByteNet[18] 23.75"
      ],
      "title": "Neural Machine Translation in Linear Time",
      "abstract": "We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.",
      "doi": null,
      "year": 2016,
      "authors": [
        "Nal Kalchbrenner",
        "L. Espeholt",
        "K. Simonyan",
        "Aäron van den Oord",
        "Alex Graves",
        "K. Kavukcuoglu"
      ],
      "citation_count": 551,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[19]",
      "ref_title": "Structured attention networks",
      "citation_contexts": [
        "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2,19]."
      ],
      "title": "Structured Attention Networks",
      "abstract": "Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Yoon Kim",
        "Carl Denton",
        "Luong Hoang",
        "Alexander M. Rush"
      ],
      "citation_count": 463,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[20]",
      "ref_title": "Adam: A method for stochastic optimization",
      "citation_contexts": [
        "We used the Adam optimizer[20] with β =0.9, β =0.98 and ǫ=10 9."
      ],
      "title": "Adam: A Method for Stochastic Optimization",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "doi": null,
      "year": 2014,
      "authors": [
        "Diederik P. Kingma",
        "Jimmy Ba"
      ],
      "citation_count": 149933,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[21]",
      "ref_title": "Factorization tricks for LSTM networks",
      "citation_contexts": [
        "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter."
      ],
      "title": "Factorization tricks for LSTM networks",
      "abstract": "We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Oleksii Kuchaiev",
        "Boris Ginsburg"
      ],
      "citation_count": 113,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[22]",
      "ref_title": "A structured self-attentive sentence embedding",
      "citation_contexts": [
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4,27,28,22]."
      ],
      "title": "A Structured Self-attentive Sentence Embedding",
      "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Zhouhan Lin",
        "Minwei Feng",
        "C. D. Santos",
        "Mo Yu",
        "Bing Xiang",
        "Bowen Zhou",
        "Yoshua Bengio"
      ],
      "citation_count": 2137,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[23]",
      "ref_title": "Multi-task sequence to sequence learning",
      "citation_contexts": [
        "Luong et al. (2015)[23] multi-task 93.0"
      ],
      "title": "Multi-task Sequence to Sequence Learning",
      "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",
      "doi": null,
      "year": 2015,
      "authors": [
        "Minh-Thang Luong",
        "Quoc V. Le",
        "I. Sutskever",
        "O. Vinyals",
        "Lukasz Kaiser"
      ],
      "citation_count": 807,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[24]",
      "ref_title": "Effective approaches to attention-based neural machine translation",
      "citation_contexts": [
        "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38,24,15]."
      ],
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",
      "doi": "10.18653/v1/D15-1166",
      "year": 2015,
      "authors": [
        "Thang Luong",
        "Hieu Pham",
        "Christopher D. Manning"
      ],
      "citation_count": 7955,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[25]",
      "ref_title": "Building a large annotated corpus of english: The penn treebank",
      "citation_contexts": [
        "We trained a 4-layer transformer with d = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank[25], about 40K training sentences."
      ],
      "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
      "abstract": null,
      "doi": null,
      "year": 1993,
      "authors": [
        "Mitchell P. Marcus",
        "Beatrice Santorini",
        "Mary Ann Marcinkiewicz"
      ],
      "citation_count": 8926,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[26]",
      "ref_title": "Effective self-training for parsing",
      "citation_contexts": [
        "McClosky et al. (2006)[26] semi-supervised 92.1"
      ],
      "title": "Effective Self-Training for Parsing",
      "abstract": "We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon.",
      "doi": "10.3115/1220835.1220855",
      "year": 2006,
      "authors": [
        "David McClosky",
        "Eugene Charniak",
        "Mark Johnson"
      ],
      "citation_count": 687,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[27]",
      "ref_title": "A decomposable attention model",
      "citation_contexts": [
        "In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.",
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4,27,28,22]."
      ],
      "title": "A Decomposable Attention Model for Natural Language Inference",
      "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.",
      "doi": "10.18653/v1/D16-1244",
      "year": 2016,
      "authors": [
        "Ankur P. Parikh",
        "Oscar Täckström",
        "Dipanjan Das",
        "Jakob Uszkoreit"
      ],
      "citation_count": 1373,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[28]",
      "ref_title": "A deep reinforced model for abstractive summarization",
      "citation_contexts": [
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4,27,28,22]."
      ],
      "title": "A Deep Reinforced Model for Abstractive Summarization",
      "abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Romain Paulus",
        "Caiming Xiong",
        "R. Socher"
      ],
      "citation_count": 1556,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[29]",
      "ref_title": "Learning accurate, compact, and interpretable tree annotation",
      "citation_contexts": [
        "Petrov et al. (2006)[29] WSJ only, discriminative 90.4",
        "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley Parser even when training only on the WSJ training set of 40K sentences."
      ],
      "title": "Learning Accurate, Compact, and Interpretable Tree Annotation",
      "abstract": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank. Starting with a simple X-bar grammar, we learn a new grammar whose nonterminals are subsymbols of the original nonterminals. In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data. Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation. On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation. Despite its simplicity, our best grammar achieves an F1 of 90.2% on the Penn Treebank, higher than fully lexicalized systems.",
      "doi": "10.3115/1220175.1220230",
      "year": 2006,
      "authors": [
        "Slav Petrov",
        "Leon Barrett",
        "R. Thibaux",
        "D. Klein"
      ],
      "citation_count": 987,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[30]",
      "ref_title": "Using the output embedding to improve language models",
      "citation_contexts": [
        "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]."
      ],
      "title": "Using the Output Embedding to Improve Language Models",
      "abstract": "We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.",
      "doi": "10.18653/V1/E17-2025",
      "year": 2016,
      "authors": [
        "Ofir Press",
        "Lior Wolf"
      ],
      "citation_count": 733,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[31]",
      "ref_title": "Neural machine translation of rare words with subword units",
      "citation_contexts": [
        "Sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations."
      ],
      "title": "Neural Machine Translation of Rare Words with Subword Units",
      "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.",
      "doi": "10.18653/v1/P16-1162",
      "year": 2015,
      "authors": [
        "Rico Sennrich",
        "B. Haddow",
        "Alexandra Birch"
      ],
      "citation_count": 7730,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[32]",
      "ref_title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "citation_contexts": [
        "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter.",
        "MoE[32] 26.03 40.56 2.0 · 1019 1.2 · 1020"
      ],
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "doi": null,
      "year": 2017,
      "authors": [
        "Noam M. Shazeer",
        "Azalia Mirhoseini",
        "Krzysztof Maziarz",
        "Andy Davis",
        "Quoc V. Le",
        "Geoffrey E. Hinton",
        "J. Dean"
      ],
      "citation_count": 2635,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[33]",
      "ref_title": "Dropout: a simple way to prevent neural networks from overfitting",
      "citation_contexts": [
        "We apply dropout[33] to the output of each sub-layer, before it is added to the sub-layer input and normalized."
      ],
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "doi": "10.5555/2627435.2670313",
      "year": 2014,
      "authors": [
        "Nitish Srivastava",
        "Geoffrey E. Hinton",
        "A. Krizhevsky",
        "I. Sutskever",
        "R. Salakhutdinov"
      ],
      "citation_count": 39787,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[34]",
      "ref_title": "End-to-end memory networks",
      "citation_contexts": [
        "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]."
      ],
      "title": "End-To-End Memory Networks",
      "abstract": null,
      "doi": null,
      "year": 2015,
      "authors": [
        "Sainbayar Sukhbaatar",
        "Arthur Szlam",
        "J. Weston",
        "R. Fergus"
      ],
      "citation_count": 2561,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[35]",
      "ref_title": "Sequence to sequence learning with neural networks",
      "citation_contexts": [
        "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35,2,5]."
      ],
      "title": "Sequence to Sequence Learning with Neural Networks",
      "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
      "doi": null,
      "year": 2014,
      "authors": [
        "I. Sutskever",
        "O. Vinyals",
        "Quoc V. Le"
      ],
      "citation_count": 20528,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[36]",
      "ref_title": "Rethinking the inception architecture for computer vision",
      "citation_contexts": [
        "Label Smoothing During training, we employed label smoothing of value ǫ = 0.1[36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."
      ],
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",
      "doi": "10.1109/CVPR.2016.308",
      "year": 2015,
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jonathon Shlens",
        "Z. Wojna"
      ],
      "citation_count": 27330,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[37]",
      "ref_title": "Grammar as a foreign language",
      "citation_contexts": [
        "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes[37].",
        "Vinyals & Kaiser el al. (2014)[37] WSJ only, discriminative 88.3",
        "Vinyals & Kaiser el al. (2014)[37] semi-supervised 92.1",
        "We trained a 4-layer transformer with d = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank[25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and Berkley Parser corpora from with approximately 17M sentences [37].",
        "In contrast to RNN sequence-to-sequence models, the Transformer outperforms the Berkeley Parser even when training only on the WSJ training set of 40K sentences."
      ],
      "title": "Grammar as a Foreign Language",
      "abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.",
      "doi": null,
      "year": 2014,
      "authors": [
        "O. Vinyals",
        "Lukasz Kaiser",
        "Terry Koo",
        "Slav Petrov",
        "I. Sutskever",
        "Geoffrey E. Hinton"
      ],
      "citation_count": 931,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[38]",
      "ref_title": "Google’s neural machine translation system: Bridging the gap between human and machine translation",
      "citation_contexts": [
        "Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38,24,15].",
        "This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9].",
        "Sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.",
        "For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary[38].",
        "GNMT+RL[38] 24.6 39.92 2.3 1019 1.4 · 1020",
        "GNMT+RLEnsemble[38] 26.30 41.16 1.8 1020 1.1 · 1021",
        "We used beam search with a beam size of 4 and length penalty α = 0.6[38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length+50, but terminate early when possible[38]."
      ],
      "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
      "doi": null,
      "year": 2016,
      "authors": [
        "Yonghui Wu",
        "M. Schuster",
        "Z. Chen",
        "Quoc V. Le",
        "Mohammad Norouzi",
        "Wolfgang Macherey",
        "M. Krikun",
        "Yuan Cao",
        "Qin Gao",
        "Klaus Macherey",
        "J. Klingner",
        "Apurva Shah",
        "Melvin Johnson",
        "Xiaobing Liu",
        "Lukasz Kaiser",
        "Stephan Gouws",
        "Yoshikiyo Kato",
        "Taku Kudo",
        "H. Kazawa",
        "K. Stevens",
        "George Kurian",
        "Nishant Patil",
        "Wei Wang",
        "C. Young",
        "Jason R. Smith",
        "Jason Riesa",
        "Alex Rudnick",
        "O. Vinyals",
        "G. Corrado",
        "Macduff Hughes",
        "J. Dean"
      ],
      "citation_count": 6783,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[39]",
      "ref_title": "Deep recurrent models with fast-forward connections for neural machine translation",
      "citation_contexts": [
        "Deep-Att+PosUnk[39] 39.2 1.0 1020",
        "Deep-Att+PosUnkEnsemble[39] 40.4 8.0 1020"
      ],
      "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
      "abstract": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT’14 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task.",
      "doi": "10.1162/tacl_a_00105",
      "year": 2016,
      "authors": [
        "Jie Zhou",
        "Ying Cao",
        "Xuguang Wang",
        "Peng Li",
        "W. Xu"
      ],
      "citation_count": 217,
      "source": "semantic scholar"
    },
    {
      "ref_number": "[40]",
      "ref_title": "Fast and accurate shift-reduce constituent parsing",
      "citation_contexts": [
        "Zhu et al. (2013)[40] WSJ only, discriminative 90.4",
        "Zhu et al. (2013)[40] semi-supervised 91.3"
      ],
      "title": "Fast and Accurate Shift-Reduce Constituent Parsing",
      "abstract": null,
      "doi": null,
      "year": 2013,
      "authors": [
        "Muhua Zhu",
        "Yue Zhang",
        "Wenliang Chen",
        "Min Zhang",
        "Jingbo Zhu"
      ],
      "citation_count": 217,
      "source": "semantic scholar"
    }
  ]
}