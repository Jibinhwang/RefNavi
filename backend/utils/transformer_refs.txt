[1] 제목: Layer normalization
참조내용: [1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXivpreprint arXiv:1607.06450,2016.

[2] 제목: Neural machine translation by jointly learning to align and translate
참조내용: [2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointly learningtoalignandtranslate. CoRR,abs/1409.0473,2014.

[3] 제목: Massive exploration of neural machine translation architectures
참조내용: [3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le. Massiveexplorationofneural machinetranslationarchitectures. CoRR,abs/1703.03906,2017.

[4] 제목: Long short-term memory-networks for machine reading
참조내용: [4] JianpengCheng,LiDong,andMirellaLapata. Longshort-termmemory-networksformachine reading. arXivpreprintarXiv:1601.06733,2016.

[5] 제목: Learning phrase representations using rnn encoder-decoder for statistical machine translation
참조내용: [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, andYoshuaBengio. Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation. CoRR,abs/1406.1078,2014.

[6] 제목: Xception: Deep learning with depthwise separable convolutions
참조내용: [6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprintarXiv:1610.02357,2016.

[7] 제목: Empirical evaluation of gated recurrent neural networks on sequence modeling
참조내용: [7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio. Empiricalevaluation ofgatedrecurrentneuralnetworksonsequencemodeling. CoRR,abs/1412.3555,2014.

[8] 제목: Convolutional sequence to sequence learning
참조내용: [8] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu- tionalsequencetosequencelearning. arXivpreprintarXiv:1705.03122v2,2017.

[9] 제목: Generating sequences with recurrent neural networks
참조내용: [9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,2013.

[10] 제목: Deep residual learning for image recognition
참조내용: [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages770–778,2016.

[11] 제목: Gradient flow in recurrent nets: the difficulty of learning long-term dependencies
참조내용: [11] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber. Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001.

[12] 제목: Long short-term memory
참조내용: [12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,1997.

[13] 제목: Exploring the limits of language modeling
참조내용: [13] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu. Exploring thelimitsoflanguagemodeling. arXivpreprintarXiv:1602.02410,2016.

[14] 제목: Neural GPUs learn algorithms
참조내용: [14] ŁukaszKaiserandIlyaSutskever. NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016.

[15] 제목: Neural machine translation in linear time
참조내용: [15] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017.

[16] 제목: Structured attention networks
참조내용: [16] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush. Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017.

[17] 제목: Adam: A method for stochastic optimization
참조내용: [17] DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InICLR,2015.

[18] 제목: Factorization tricks for LSTM networks
참조내용: [18] OleksiiKuchaievandBorisGinsburg. FactorizationtricksforLSTMnetworks. arXivpreprint arXiv:1703.10722,2017.

[19] 제목: A structured self-attentive sentence embedding
참조내용: [19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130,2017.

[20] 제목: Can active memory replace attention?
참조내용: [20] SamyBengioŁukaszKaiser. Canactivememoryreplaceattention? InAdvancesinNeural InformationProcessingSystems,(NIPS),2016.

[21] 제목: Effective approaches to attention-based neural machine translation
참조내용: [21] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention- basedneuralmachinetranslation. arXivpreprintarXiv:1508.04025,2015.

[22] 제목: A decomposable attention model
참조내용: [22] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention model. InEmpiricalMethodsinNaturalLanguageProcessing,2016.

[23] 제목: A deep reinforced model for abstractive summarization
참조내용: [23] RomainPaulus,CaimingXiong,andRichardSocher. Adeepreinforcedmodelforabstractive summarization. arXivpreprintarXiv:1705.04304,2017.

[24] 제목: Using the output embedding to improve language models
참조내용: [24] OfirPressandLiorWolf. Usingtheoutputembeddingtoimprovelanguagemodels. arXiv preprintarXiv:1608.05859,2016.

[25] 제목: Neural machine translation of rare words with subword units
참조내용: [25] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslationofrarewords withsubwordunits. arXivpreprintarXiv:1508.07909,2015.

[26] 제목: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer
참조내용: [26] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, andJeffDean. Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts layer. arXivpreprintarXiv:1701.06538,2017.

[27] 제목: Dropout: a simple way to prevent neural networks from overfitting
참조내용: [27] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. JournalofMachine LearningResearch,15(1):1929–1958,2014.

[28] 제목: End-to-end memory networks
참조내용: [28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates, Inc.,2015.

[29] 제목: Sequence to sequence learning with neural networks
참조내용: [29] IlyaSutskever,OriolVinyals,andQuocVVLe. Sequencetosequencelearningwithneural networks. InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.

[30] 제목: Rethinking the inception architecture for computer vision
참조내용: [30] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Rethinkingtheinceptionarchitectureforcomputervision. CoRR,abs/1512.00567,2015.

[31] 제목: Google’s neural machine translation system: Bridging the gap between human and machine translation
참조내용: [31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal. Google’sneuralmachine translationsystem: Bridgingthegapbetweenhumanandmachinetranslation. arXivpreprint arXiv:1609.08144,2016.

[32] 제목: Deep recurrent models with fast-forward connections for neural machine translation
참조내용: [32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forwardconnectionsforneuralmachinetranslation. CoRR,abs/1606.04199,2016.

